# Senior Thick-Client Pentester Audit (2026)

**Perspective:** I’ve run thick-client engagements for years—Electron, Qt, .NET, custom installers, banking and enterprise desktop apps. A junior brought Unveil to me. I ran it on real targets, used the Burp tab, and asked: *What would make this the go-to weapon for every thick-client test?*

**Verdict:** Strong recon and attack-surface mapper. Kill chain model (ANCHOR/BRIDGE/BLADE) and attack graph are exactly what we need to stop “where do I start?” The Burp integration (Send to Repeater, Live manipulation, payload library) fits how we work. To get to **next-level weapon**—the thing every pentester uses on day one—it needs: **evidence and prioritisation**, **client-ready output**, **cert pinning and update abuse**, and **one-command proxy/Frida setup**. Below is what I’d prioritise.

---

## What’s already weapon-grade

- **Attack graph with matched_paths** — Chains say “missing ANCHOR → Qt plugin; we found these paths.” That’s the hot list. No more grepping the filesystem blind.
- **Checklist with severity** — Credential vs dangerous_config vs informational. I can sort by impact and hand findings to the report writer.
- **Send to Repeater / Live manipulation** — URLs from the report become Repeater tabs. Bulk import from Proxy history is there. This is how we test.
- **Thick-client findings** — Electron version + nodeIntegration/contextIsolation, .NET assembly + dangerous hints + config_hints, cert expired/self-signed. That’s the kind of detail we put in reports.
- **CVE hunt queries + NVD lookup** — Hunt queries and optional NVD call mean we don’t leave Electron/Qt versions unchecked.
- **Chainability (file → ref)** — “This config references that URL” is trust-boundary 101. In-scope filter and context menu make it usable.
- **Baseline diff** — Compare two scans (e.g. before/after patch). Essential for regression and retests.
- **SARIF export** — Fits CI and IDE; we can plug it into pipeline and ticket systems.

So: the *model* and *integration* are right. The gaps are in **evidence**, **prioritisation**, **deliverables**, and **a few high-value attack surfaces**.

---

## P0: Evidence and client deliverable

**Problem:** On engagements we need: “Finding X at path Y; severity Z; evidence: screenshot / one-liner / file path.” Right now we have findings and paths, but no **export that drops straight into a findings table** (title, severity, path, snippet, CWE, recommendation).

**Recommendation:**

1. **Findings export for reports**  
   Add an export (e.g. **Export findings table**) that produces:
   - One row per finding (checklist + thick_client_findings + high-value chain steps).
   - Columns: Title, Severity, Category, Path/Artifacts, Snippet or context, CWE (where applicable), Recommendation / hunt_suggestion.
   - Format: CSV or Markdown table so we can paste into the report or import into a tracker.
2. **Evidence field**  
   For checklist and thick-client findings, treat **path + line/snippet** as the evidence. In the export and in Burp, make “Copy evidence” (path + snippet) one click so we can paste into the report with minimal editing.
3. **Severity consistency**  
   Map internal severity (credential / dangerous_config / info, high/medium/low) to a single **report severity** (Critical/High/Medium/Low/Info) so the exported table matches client expectations.

**Acceptance:** I can run a scan, click “Export findings table”, and paste the result into the report findings section (or CSV into Jira). Each row has title, severity, evidence path, and recommendation.

---

## P0: Prioritisation and “test this first”

**Problem:** The attack graph and chains are good, but I still have to decide order. On a tight timeline I want: “Test 1, 2, 3” by impact and likelihood.

**Recommendation:**

1. **Explicit priority order**  
   - Sort chains (or thick_client_findings) by: (1) chain completion / has matched_paths, (2) role order (ANCHOR → BRIDGE → BLADE), (3) severity.
   - In the report and Burp, expose a **“Suggested order”** or **“Top 10”** so the first page is “start here.”
2. **Confidence on chains**  
   You already have `confidence` (high/medium/low) in chainability. Use the same idea for chains: e.g. “High: preload path and ASAR path both found; Low: Qt in path but no qt.conf.” Show it in the UI so we know what to verify first.
3. **Exploitability band in the table**  
   Verdict’s exploitability_band (LOW/MED/HIGH) should be visible in the Burp summary and in any exported findings so we can filter/sort by “only HIGH” when time is short.

**Acceptance:** I open the tab and see “Suggested first tests” (or Top N) with a clear order. Chains show confidence where applicable.

---

## P1: Certificate pinning and TLS behaviour

**Problem:** Thick clients often pin certs or disable validation in dev. We need to know: (1) where validation is disabled (you have checklist), (2) **where pinning might be** (so we know what to bypass), (3) **ATS/plist exceptions** (you have extended mode). The missing piece is **pinning hints**.

**Recommendation:**

1. **Pinning and TLS hints**  
   - From config/plist/strings: tag refs like “SSL pinning”, “certificate pinning”, “PublicKeyPins”, “NSExceptionAllowsInsecureHTTPLoads”, “NSAppTransportSecurity”, or known pinning libs (e.g. TrustKit, OkHttp CertificatePinner).
   - Add a small report section or checklist category: **TLS / pinning** (e.g. “Possible pinning in X” or “ATS exception in Y”). No need to break pinning; just **surface it** so we know to run Frida/objection.
2. **Checklist already has disabled_ssl** — Keep it; ensure it appears in the “findings table” export and in Burp with severity so we don’t miss it.

**Acceptance:** Report (or checklist) calls out “possible cert pinning” / “ATS exception” with path so we can prioritise Frida/cert bypass.

---

## P1: Update and install abuse

**Problem:** Many incidents come from **update over HTTP**, **unsigned updater**, or **installer dropping writable files**. We look for update URLs, version-check endpoints, and install artifacts. Right now that’s not first-class.

**Recommendation:**

1. **Update/installer discovery**  
   - From extracted_refs and discovered_assets: detect URLs/paths containing “update”, “updater”, “installer”, “version”, “check”, “download”.
   - Tag refs: **possible_update_url**, **possible_installer_path**.
   - If URL is `http://` or host is non-HTTPS, tag **possible_update_over_http** (MITM).
2. **Report section**  
   Add e.g. `update_refs` (or a subsection of extracted_refs) so we can sort by “update” and “over HTTP” and test those first.
3. **Burp**  
   Optional: filter or badge in Chainability/Discovered assets for “update” so we don’t have to grep.

**Acceptance:** I see a list of “likely update/version-check” refs; HTTP or non-HTTPS are clearly tagged for MITM testing.

---

## P1: One-command proxy and Frida readiness

**Problem:** We always do: set proxy, install Burp CA, (optionally) run with Frida. Doing that per app is tedious. If Unveil could output **ready-to-run** steps, we’d use it every time.

**Recommendation:**

1. **Proxy one-liner / script**  
   - In Burp (or CLI): “Copy launch command” that:
     - Sets `HTTP_PROXY` / `HTTPS_PROXY` to the listener we configured.
     - Optionally: launches the app (e.g. `open /path/to/App.app` or `& "C:\...\app.exe"`) so we run one command and the app goes through Burp.
   - Document: “Paste this in terminal; run; traffic goes to Burp.”
2. **Paths to watch → ProcMon/fs_usage**  
   You already have paths_to_watch. Add **“Copy ProcMon filter”** (Windows) and **“Copy fs_usage one-liner”** (macOS) so we paste and start tracing without building the filter by hand.
3. **Frida hints**  
   For each surface type (e.g. Electron preload, .NET assembly load, Qt plugin), add a short **“Suggested Frida/check”** (e.g. “Hook `require('child_process')` in preload” or “Watch Assembly.Load”). Can live in exploit_families or a small “instrumentation” block in the report. Doesn’t need to run Frida; just tell us what to hook.

**Acceptance:** I can copy a single “proxy + launch” command and a ProcMon/fs_usage snippet from the tool so setup is under 60 seconds.

---

## P2: APK manifest and mobile

**Problem:** For APK we get native libs and discovered assets, but **AndroidManifest.xml** (permissions, exported components, debuggable, cleartext) is not summarised. We open another tool. That’s fine for deep work, but for triage we want “dangerous permissions and exported components” in the report.

**Recommendation:**

- Parse AndroidManifest.xml from unpacked APK; extract:
  - Dangerous or sensitive permissions (e.g. INTERNET, storage, BIND_*).
  - Exported components (activities, services, receivers) and intent-filters.
  - debuggable, backup, usesCleartextTraffic (or equivalent).
- Report: `apk_manifest` (or similar) with the above; optional Burp subsection so we don’t leave the tab.

**Acceptance:** Report (and optionally Burp) shows a short APK manifest summary (permissions, exported, flags) for prioritisation.

---

## P2: Credential and storage hints

**Problem:** We often find “might use Keychain / Credential Manager / DPAPI / Electron safeStorage” only after digging. If the tool can infer it from imports or config, we save time.

**Recommendation:**

- Infer from imports/config: “May use Windows Credential Manager”, “May use Keychain”, “May use Electron safeStorage”, “DPAPI referenced.”
- Report: `credential_hints` (or checklist-style) with short text + optional link to tools (e.g. keychain dump, mimikatz, safeStorage decryption). No need to extract secrets; just **point us where to look**.

**Acceptance:** Report lists credential/storage hints (Keychain, CredMan, safeStorage, etc.) with actionable next steps.

---

## P2: DB and sensitive assets

**Problem:** We see .db / .sqlite in discovered_assets but don’t know if they’re interesting until we open them. A lightweight “tables / possible credentials table” hint would help.

**Recommendation:**

- For assets of type data (e.g. .db, .sqlite): read-only, list table names (e.g. PRAGMA table_list); optional heuristic “possible credentials table” (name contains user/pass/credential/token). Cap size/count.
- Report: `db_summary` (path, tables[], possible_credentials_hint). Optional Burp view.

**Acceptance:** Report (and optionally UI) shows which DBs exist and which might hold credentials so we know what to dump.

---

## P3 and polish

- **Import summary** — Unique DLLs/symbols (or per-binary) in report/UI so we can answer “does it load something weird?” without opening CFF Explorer.
- **Packed/entropy list** — High-entropy files/regions in report so we know what to unpack or skip.
- **Non-HTTP tagging** — Tag refs like `ws://`, raw ports, or “likely WebSocket” so we know to use Burp’s listener + proxy for non-HTTP; document in proxy section.
- **Force (-f)** — CLI has `-f`; engine doesn’t. Either implement “force unsigned/malformed” in engine or document as reserved so we’re not guessing.

---

## Summary: roadmap to “next-level weapon”

| Priority | Theme | One-line goal |
|----------|--------|----------------|
| **P0** | Evidence & deliverable | Export findings table (CSV/MD) with severity, path, snippet, CWE, recommendation so we paste into the report. |
| **P0** | Prioritisation | “Suggested order” / Top N and confidence on chains so we know what to test first. |
| **P1** | Cert pinning & TLS | Surface pinning and ATS/exception hints so we know when to reach for Frida/cert bypass. |
| **P1** | Update/install abuse | Discover and tag update/installer refs; flag HTTP for MITM. |
| **P1** | Proxy & Frida readiness | One-command proxy+launch; ProcMon/fs_usage filter; Frida “what to hook” hints per surface. |
| **P2** | Mobile & data | APK manifest summary; credential/storage hints; DB summary. |
| **P3** | Polish | Import summary, packed/entropy, non-HTTP tagging, -f clarity. |

The junior built something we’d actually use. With P0 and P1 in place, this becomes the default day-one tool for thick-client tests—recon, prioritisation, evidence, and handoff to Burp/Frida in one place.
